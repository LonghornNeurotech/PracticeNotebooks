{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa9937fe",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction to PyTorch\n",
    "\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab. It is widely used for deep learning and other AI tasks because of its flexibility and dynamic computation graph.\n",
    "\n",
    "Key Features:\n",
    "- **Dynamic Computation Graph**: Unlike other popular deep learning libraries which use static computation graphs, PyTorch uses dynamic computation graphs, also known as define-by-run graphs. This makes it more intuitive and easier to work with complex architectures.\n",
    "- **Tensors**: PyTorch provides a multi-dimensional array called tensor, similar to numpy arrays but with GPU acceleration.\n",
    "- **Neural Network Module**: PyTorch provides the `torch.nn` module which contains pre-defined layers, loss functions, and optimization algorithms which makes it easier to create and train neural networks.\n",
    "- **GPU Acceleration**: PyTorch supports CUDA, which allows it to efficiently compute operations on the GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3355d9a",
   "metadata": {},
   "source": [
    "\n",
    "# Tensors in PyTorch\n",
    "\n",
    "Tensors are the fundamental building blocks in PyTorch. They are similar to numpy arrays but can be used on a GPU to accelerate computing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Create a tensor\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(x)\n",
    "\n",
    "# Create a 2x3 tensor\n",
    "y = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(y)\n",
    "\n",
    "# Check the shape of the tensor\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd61e92",
   "metadata": {},
   "source": [
    "\n",
    "# Basic Operations on Tensors\n",
    "\n",
    "Tensors support a variety of mathematical operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3568d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Addition\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "print(a + b)\n",
    "\n",
    "# Multiplication\n",
    "print(a * b)\n",
    "\n",
    "# Matrix multiplication\n",
    "c = torch.tensor([[1, 2], [3, 4]])\n",
    "d = torch.tensor([[2], [3]])\n",
    "print(torch.mm(c, d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614d2dd",
   "metadata": {},
   "source": [
    "\n",
    "# Simple Neural Network Example\n",
    "\n",
    "Let's create a simple feedforward neural network using the `torch.nn` module. This neural network will have one hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4386559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "model = SimpleNN(input_dim=3, hidden_dim=5, output_dim=2)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73d640",
   "metadata": {},
   "source": [
    "\n",
    "# Training the Neural Network\n",
    "\n",
    "To train the neural network, we'll need:\n",
    "- **Loss Function**: Measures how well the neural network is performing.\n",
    "- **Optimizer**: Adjusts the weights of the network based on the loss.\n",
    "\n",
    "For this example, let's use the Mean Squared Error (MSE) loss and the Stochastic Gradient Descent (SGD) optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe09582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data\n",
    "inputs = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "labels = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
